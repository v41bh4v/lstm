# Recurrent Neural Networks with LSTM
lstm-via- orquestra

This tutorial will walk through the implementation of a neural network model that can learn how to reconstruct the shape of a noisy sine function using Recurrent Neural Networks (RNNs).

RNNs are one of the canonical approaches for dealings with sequence problems since they can retain the state from one iteration to the next by using their own output as additional input for the next step, i.e., the input for RNNs correspond to the data and the state of the network one previous step. Simple RNNs suffer from the fundamental problem of not being able to capture long-term dependencies in a sequence, and to tackle that issue in the late ’90s, the so-called Long Short-Term Memory networks (LSTMs) were proposed by Sepp Hochreiter and Jurgen Schmidhuber. This exercise will make use of LSTMs as neurons type in the model.

The problem considered here belongs to the category of supervised learning since it involves learning a function that maps a set of inputs to one output based on multiple instances of (input, output) pairs. In the following section, it is stated the input and output datasets of the problem at hand.

Input and output dataset
The raw dataset consists of a noisy sine function data, generated by adding up an analytical sine function and a noisy signal sampled from a normal distribution with mean zero and standard deviation of one. The parameters that control the data generation are the following:

time-range: the upper limit of the time range of data to be generated. The time range starts at 0. The time_range is not included as the last point.
time-step: the step between each of the time values.
noise-std: the standard deviation of the noise. As mentioned above, the noise follows a normal distribution centered at zero.
The generated data looks like the plot below:



As explained later, this raw data will be split into training and testing datasets, and both of them will be subjected to an operation that creates the pair values of (input, output) of the supervised problem. In this respect, the x-values (inputs) correspond to a time rolling window of specified length, and the y-values (outputs) to the next value to be predicted. Below there is a snippet of the code realizing that operation.

The series of (input, output) pairs for supervising learning are built in the following manner:

Select a lookback window value, which will represent the number of previous time steps values that will be used as information for predicting the next value.
Create x-values: split up the data in batches of size lookback window above in a rolling window manner across all available data.
Create y-values: pick up the next time step value data for each of the batches of size lookback window above.
You can find a snippet of the code to generate the series of (input, output) pairs.


  # Create pairs of a window of data and the next value after the window
  xs, ys = [], []
  for i in range(len(x) - window_size):
      v = x.iloc[i:(i + window_size)].values
      xs.append(v)
      ys.append(y.iloc[i + window_size])

    return np.array(xs), np.array(ys)
Neural network model
RNNs model may be considered a computational black box that takes in a number of sequential values within a lookback window of values and forecasts the next value. In this exercise, the type of RNN used is that of LSTM, which is explicitly designed to capture long-term dependencies in sequential problems. Below there is an illustration of the computation that each LSTM represents.



where the x’s represent the set of values of size lookback window that corresponds to the input data, and the h’s are the state values of the neurons in the RNN layer, which summarize the information from all the previous steps within the lookback window seen thus far. Please note that, as is standard practice, the initialization of the hidden layer output is set up to zero, i.e., h0 = 0. Last, y represents the output data to be predicted.

Therefore, the idea of an RNN model as a computational black box can be substantiated by looking at the NN model as a machine accepting a series of inputs, x’s, to produce an output y.

The model here used consists of a neural network with a single hidden layer of LSTM neurons and a final Dense layer for the output, where Dense in this context means that all neurons from the previous layer are connected to the current one. In this case, the output layer with one single neuron, which corresponds to the y predicted value. Additionally, to prevent overfitting in the training process, a Dropout operation is added to the LSTM layer, which can be seen as a form of regularization.

The code to create the model reads as follows:

# Function to build the model
def build_model(hnodes=32, dropout=0.2):
    model = keras.Sequential()

    # Adding one LSTM layer
    model.add(keras.layers.LSTM(
        units=hnodes,
        input_shape=(xtrain.shape[1], xtrain.shape[2])
    ))

    # Adding dropout
    model.add(keras.layers.Dropout(dropout))

    # Adding a Dense layer at the end
    model.add(keras.layers.Dense(units=1))

    # Compile model using MSE as loss function to minimize, and Adam as optimiser
    model.compile(
        loss='mean_squared_error',
        optimizer=keras.optimizers.Adam(learning_rate)
    )
After having created the model, it is possible to print out its main characteristics, such as layers and the number of their corresponding parameters. The table shown below can be generated with the Keras command {your_model_name}.summary(), which summarizes the model:



It is also possible to plot the model by means of importing package from tensorflow.keras.utils import plot_model, and by using the command plot_model({your_model_name}, to_file='{plot_name}.png', how_shapes=True, show_layer_names=False). The output for this model is shown below:



The meaning of the values in parenthesis is the following:

(?, 10, 1): the first location corresponds to the size of the training dataset, and ? states that the number is open, and does not need to be explicitly stated; the second position is the lookback window, which in this problem was set up to 10; finally, the third position means the number of input sequences, which it is just 1 in this problem.
(?, 128): the first position corresponds, as above, to the size of the training dataset; and the second position to the number of neurons in the layer, i.e., 128 LSTM neurons on this hidden layer of the problem.
(?, 1)): the first position, as above, is the size of the training dataset; and the second corresponds to the number of output neurons, in this case just 1, as the model predicts only one value, i.e., the next time step value.
Now, it can be stated how the RNN model can be used to solve the supervised learning problem in hand:

Initialize the values of all the parameters involved in the RNN model.
Use a value, x, from each of the values on the left side in (input, output) pairs in the dataset.
Pass that input into the RNN model produces a predicted value, y, out of it.
Compare the predicted y value from the y value from the right side in the pairs (input, output) in the dataset, and compute the error between them.
Update the parameters of the model to minimize (optimize) the error calculated above.
Therefore, to complete the model, and to enable it to learn from data, it is required to specify a Loss function, which measures the deviation between the predicted values of the model and the ground truth values (the y values from the dataset pairs), and an Optimizer as the computational object to drive the search in parameter space aims to minimize that deviation. In the implementation for this problem, we use the mean squared error (MSE) as the loss function, and an extension of stochastic gradient descent, named Adam, as the optimizer. Below is the MSE loss function, and the snippet of the code to build the model.



As part of this tutorial, we save the model’s architecture and weights as a JSON file. This allows us to use this model as an Orquestra artifact which can be later utilized a posteriori as a black-box predictor, or even as initialization condition for further training, as in the case of transfer learning, for instance.

Test-train split
To train the model the full original dataset is first divided into training and testing sets. The first dataset will be used for training the model, and the second one to check the generalization capability of the model. In general terms:

The train dataset is used to train the model, i.e., weights and biases in the LSTM layers. The model learns from this data.

The valuation dataset is a percentage of the training dataset used to provide an unbiased evaluation of the model along the training process. It is a means to assess a potential overfitting of the model. This percentage value is a hyperparameter of the model, which in our example is initially set up to 0.1, i.e., 10% of the training data.

The test dataset is used to provide an unbiased evaluation of the model. It is only used once a model is completely trained.



Below is a snippet of the code implementing the splitting operation of the raw data into training and test.

# Splitting up dataset into training and testing datsets
#   trainperc: percentage of raw dataset to go to training set
#   dfsize: size of raw dataset
train_size = int(dfsize * trainperc)
test_size = dfsize - train_size
train, test = df.iloc[0:train_size], df.iloc[train_size:dfsize]
Plotting prediction vs. test values
Below is a plot representing the training history, i.e., the loss function (MSE), defined at the time of model compilation above, both during validation and training stages. It can be noted that the validation error is less than the training error, which can be understood as the training error is calculated for the entire epoch - where at its beginning is worse than at the end -, whereas the validation error is taken from the last batch - after the model is improved by the training process.

